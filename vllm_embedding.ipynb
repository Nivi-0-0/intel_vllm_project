{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaad66d-7164-421a-be12-5b8d67e23630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\clip_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#run this\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f56570-f091-412e-b71c-960e0ee2c936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\clip_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\clip_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\clip_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\clip_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\clip_env\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: six>=1.5 in e:\\clip_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a69920-5297-46f2-abe8-55d84d5759f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unzipping Flickr8k_Dataset.zip...\n",
      "✅ Done extracting!\n"
     ]
    }
   ],
   "source": [
    "#run this\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Set path to your uploaded ZIP file and output folder\n",
    "zip_path = Path(\"flickr_data/Flickr8k_Dataset.zip\")  # change if it's elsewhere\n",
    "extract_to = Path(\"flickr_data\")  # you can change this too\n",
    "\n",
    "# Function to unzip\n",
    "def unzip(zip_path, extract_to):\n",
    "    print(f\"📦 Unzipping {zip_path.name}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"✅ Done extracting!\")\n",
    "\n",
    "# Call it\n",
    "unzip(zip_path, extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78665ae-9bd5-4db1-83e5-29e5df81432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All image-caption pairs:                        image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1001773457_577c3a7d70.jpg   \n",
      "2  1002674143_1b742ab4b8.jpg   \n",
      "3  1003163366_44323f5815.jpg   \n",
      "4  1007129816_e794419615.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  [A child in a pink dress is climbing up a set ...  \n",
      "1  [A black dog and a spotted dog are fighting, A...  \n",
      "2  [A little girl covered in paint sits in front ...  \n",
      "3  [A man lays on a bench while his dog sits by h...  \n",
      "4  [A man in an orange hat starring at something ...  \n"
     ]
    }
   ],
   "source": [
    "#run\n",
    "import pandas as pd\n",
    "\n",
    "# Set your paths\n",
    "CAPTION_FILE = \"flickr_data/Flickr8k.token.txt\"\n",
    "IMG_DIR = \"flickr_data/Flicker8k_Dataset\"  # spelling check here!\n",
    "\n",
    "# Load captions\n",
    "with open(CAPTION_FILE, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse captions into a DataFrame\n",
    "data = []\n",
    "for line in lines:\n",
    "    img_caption = line.strip().split('\\t')\n",
    "    img_filename = img_caption[0].split('#')[0]  # Getting the filename (e.g., image.jpg)\n",
    "    caption = img_caption[1]\n",
    "    data.append((img_filename, caption))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"image\", \"caption\"])\n",
    "\n",
    "# Group captions by image\n",
    "grouped = df.groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
    "\n",
    "# Now, we have all 1000 image-caption pairs\n",
    "print(\"✅ All image-caption pairs:\", grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20e9684-1666-484a-936c-70cb3c37f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torchvision faiss-cpu gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bccd643-1d8e-4186-ad74-493ac268096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)  # This will exit the kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447b76c6-f101-4d43-a10a-ed340f204fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model from OpenAI\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "model.eval()  # set to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c889980-3391-4041-8e58-42d5878a83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hakre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a23e3f-fb30-4c8d-9eda-9f1019193995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)  # This will exit the kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fab7ad-57ee-4602-bfc2-1685cb70b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dir exists: True\n",
      "Token file exists: True\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"E:/intel/flickr_data/Flicker8k_Dataset\"\n",
    "token_path = \"E:/intel/flickr_data/Flickr8k.token.txt\"\n",
    "import os\n",
    "\n",
    "print(\"Image dir exists:\", os.path.exists(img_dir))  # should be True\n",
    "print(\"Token file exists:\", os.path.exists(token_path))  # should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aba160-6cdb-4acf-a0cd-e9b7f56891cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total pairs available: 40455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|████████████████████████████████████████████████████| 500/500 [04:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 10 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 11: 100%|████████████████████████████████████████████████████| 500/500 [04:47<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 11 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 12: 100%|████████████████████████████████████████████████████| 500/500 [03:29<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 12 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 13: 100%|████████████████████████████████████████████████████| 500/500 [03:50<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 13 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 14: 100%|████████████████████████████████████████████████████| 500/500 [04:12<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 14 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 15: 100%|████████████████████████████████████████████████████| 500/500 [04:31<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 15 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 16: 100%|████████████████████████████████████████████████████| 500/500 [04:23<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 16 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 17: 100%|████████████████████████████████████████████████████| 500/500 [04:09<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 17 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 18: 100%|████████████████████████████████████████████████████| 500/500 [04:47<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 18 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 19: 100%|████████████████████████████████████████████████████| 500/500 [03:57<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 19 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 20: 100%|████████████████████████████████████████████████████| 500/500 [04:02<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 20 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 21: 100%|████████████████████████████████████████████████████| 500/500 [05:24<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 21 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 22: 100%|████████████████████████████████████████████████████| 500/500 [05:32<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 22 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 23: 100%|████████████████████████████████████████████████████| 500/500 [04:35<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 23 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 24: 100%|████████████████████████████████████████████████████| 500/500 [05:03<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 24 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 25: 100%|████████████████████████████████████████████████████| 500/500 [04:46<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 25 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 26: 100%|████████████████████████████████████████████████████| 500/500 [05:34<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 26 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 27: 100%|████████████████████████████████████████████████████| 500/500 [06:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 27 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 28: 100%|████████████████████████████████████████████████████| 500/500 [06:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 28 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 29: 100%|████████████████████████████████████████████████████| 500/500 [06:10<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 29 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 30: 100%|████████████████████████████████████████████████████| 500/500 [05:38<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 30 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 31: 100%|████████████████████████████████████████████████████| 500/500 [05:34<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 31 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 32: 100%|████████████████████████████████████████████████████| 500/500 [06:01<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 32 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 33: 100%|████████████████████████████████████████████████████| 500/500 [06:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 33 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 34: 100%|████████████████████████████████████████████████████| 500/500 [05:59<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 34 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 35: 100%|████████████████████████████████████████████████████| 500/500 [06:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 35 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 36: 100%|████████████████████████████████████████████████████| 500/500 [06:16<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 36 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 37: 100%|████████████████████████████████████████████████████| 500/500 [06:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 37 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 38: 100%|████████████████████████████████████████████████████| 500/500 [05:18<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 38 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 39: 100%|████████████████████████████████████████████████████| 500/500 [03:48<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 39 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 40: 100%|████████████████████████████████████████████████████| 500/500 [04:55<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 40 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 41: 100%|████████████████████████████████████████████████████| 500/500 [05:48<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 41 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 42: 100%|████████████████████████████████████████████████████| 500/500 [06:05<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 42 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 43: 100%|████████████████████████████████████████████████████| 500/500 [06:12<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 43 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 44: 100%|████████████████████████████████████████████████████| 500/500 [05:46<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 44 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 45: 100%|████████████████████████████████████████████████████| 500/500 [05:49<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 45 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 46: 100%|████████████████████████████████████████████████████| 500/500 [06:17<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 46 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 47: 100%|████████████████████████████████████████████████████| 500/500 [06:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 47 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 48: 100%|████████████████████████████████████████████████████| 500/500 [06:16<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 48 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 49: 100%|████████████████████████████████████████████████████| 500/500 [06:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 49 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 50: 100%|████████████████████████████████████████████████████| 500/500 [06:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 50 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 51: 100%|████████████████████████████████████████████████████| 500/500 [06:19<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 51 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 52: 100%|████████████████████████████████████████████████████| 500/500 [05:53<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 52 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 53: 100%|████████████████████████████████████████████████████| 500/500 [06:17<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 53 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 54: 100%|████████████████████████████████████████████████████| 500/500 [06:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 54 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 55: 100%|████████████████████████████████████████████████████| 500/500 [06:14<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 55 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 56: 100%|██████████████████████████████████████████████████| 500/500 [3:10:28<00:00, 22.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 56 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 57: 100%|████████████████████████████████████████████████████| 500/500 [05:06<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 57 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 58: 100%|████████████████████████████████████████████████████| 500/500 [04:29<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 58 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 59: 100%|████████████████████████████████████████████████████| 500/500 [03:49<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 59 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 60: 100%|████████████████████████████████████████████████████| 500/500 [04:14<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 60 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 61: 100%|████████████████████████████████████████████████████| 500/500 [04:16<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 61 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 62: 100%|████████████████████████████████████████████████████| 500/500 [04:06<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 62 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 63: 100%|████████████████████████████████████████████████████| 500/500 [03:51<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 63 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 64:  39%|████████████████████▍                               | 196/500 [01:26<02:39,  1.90it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Set paths\n",
    "img_dir = \"E:/intel/flickr_data/Flicker8k_Dataset\"\n",
    "token_path = \"E:/intel/flickr_data/Flickr8k.token.txt\"\n",
    "save_dir = \"E:/intel/flickr_embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load token file\n",
    "with open(token_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse into (img_path, caption) pairs\n",
    "pairs = []\n",
    "for line in lines:\n",
    "    img_id, caption = line.strip().split('\\t')\n",
    "    img_file = img_id.split('#')[0]\n",
    "    img_path = os.path.join(img_dir, img_file)\n",
    "    if os.path.exists(img_path):\n",
    "        pairs.append((img_path, caption))\n",
    "\n",
    "print(f\"✅ Total pairs available: {len(pairs)}\")\n",
    "\n",
    "# Batch loop — resume from batch 9\n",
    "batch_size = 500\n",
    "start_batch = 9\n",
    "\n",
    "for i in range(start_batch, (len(pairs) // batch_size) + 1):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch = pairs[start:end]\n",
    "\n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "\n",
    "    for img_path, caption in tqdm(batch, desc=f\"Batch {i+1}\"):\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(text=caption, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            image_embed = outputs.image_embeds.detach().cpu()\n",
    "            text_embed = outputs.text_embeds.detach().cpu()\n",
    "\n",
    "            image_embeddings.append(image_embed)\n",
    "            text_embeddings.append(text_embed)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {img_path}: {e}\")\n",
    "\n",
    "    if image_embeddings:\n",
    "        torch.save(torch.cat(image_embeddings), os.path.join(save_dir, f\"image_embeddings_batch_{i+1}.pt\"))\n",
    "    if text_embeddings:\n",
    "        torch.save(torch.cat(text_embeddings), os.path.join(save_dir, f\"text_embeddings_batch_{i+1}.pt\"))\n",
    "\n",
    "    print(f\"✅ Saved batch {i+1} embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d663d6-fd7e-4b00-b7e0-1abcc2320e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set paths\n",
    "img_dir = \"E:/intel/flickr_data/Flicker8k_Dataset\"\n",
    "token_path = \"E:/intel/flickr_data/Flickr8k.token.txt\"\n",
    "\n",
    "# Load all image paths\n",
    "img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n",
    "\n",
    "# Load captions into a dict: {filename: [list of captions]}\n",
    "caption_dict = {}\n",
    "with open(token_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        fname, caption = line.strip().split(\"\\t\")\n",
    "        fname = fname.split(\"#\")[0]  # remove the #0, #1, etc.\n",
    "        if fname not in caption_dict:\n",
    "            caption_dict[fname] = []\n",
    "        caption_dict[fname].append(caption)\n",
    "\n",
    "# Build all_images and all_texts using one caption per image\n",
    "all_images = []\n",
    "all_texts = []\n",
    "\n",
    "for img_path in img_paths:\n",
    "    fname = os.path.basename(img_path)\n",
    "    if fname in caption_dict:\n",
    "        all_images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "        all_texts.append(caption_dict[fname][0])  # Use first caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a5ecade-bc6c-4963-a01f-64861cf1cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 image embedding files.\n",
      "Found 8 text embedding files.\n",
      "✅ Loaded image embeddings shape: torch.Size([4000, 512])\n",
      "✅ Loaded text embeddings shape: torch.Size([4000, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "\n",
    "img_paths = sorted(glob.glob(\"E:/intel/image_embeddings_batch_*.pt\"))\n",
    "txt_paths = sorted(glob.glob(\"E:/intel/text_embeddings_batch_*.pt\"))\n",
    "\n",
    "print(f\"Found {len(img_paths)} image embedding files.\")\n",
    "print(f\"Found {len(txt_paths)} text embedding files.\")\n",
    "\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "# Load image embeddings\n",
    "for p in img_paths:\n",
    "    emb = torch.load(p)\n",
    "    if emb.numel() > 0:\n",
    "        image_embeddings.append(emb)\n",
    "\n",
    "# Load text embeddings\n",
    "for p in txt_paths:\n",
    "    emb = torch.load(p)\n",
    "    if emb.numel() > 0:\n",
    "        text_embeddings.append(emb)\n",
    "\n",
    "import torch\n",
    "\n",
    "image_embeddings = torch.cat(image_embeddings)\n",
    "text_embeddings = torch.cat(text_embeddings)\n",
    "\n",
    "print(\"✅ Loaded image embeddings shape:\", image_embeddings.shape)\n",
    "print(\"✅ Loaded text embeddings shape:\", text_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9863b03-1787-423c-bc20-e0ab442e8f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 FAISS index created and saved!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "\n",
    "# Convert to float32 numpy (FAISS only supports float32)\n",
    "img_emb_np = image_embeddings.detach().cpu().numpy().astype('float32')\n",
    "\n",
    "# Create the FAISS index\n",
    "d = img_emb_np.shape[1]  # embedding dimension\n",
    "index = faiss.IndexFlatL2(d)  # L2 = Euclidean distance\n",
    "index.add(img_emb_np)  # add image embeddings\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"E:/intel/flickr_faiss_index.idx\")\n",
    "\n",
    "print(\"🎯 FAISS index created and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3c894f-cd07-4d99-81f7-2cbf36298938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Clear unused variables\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m image_embeddings, text_embeddings\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Garbage collection\u001b[39;00m\n\u001b[0;32m      8\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear unused variables\n",
    "del image_embeddings, text_embeddings\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Empty GPU cache if using CUDA (not likely on CPU but just in case)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ RAM cleaned up!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55e53f3-6b4a-4172-a214-135595859da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\clip_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf8ea80-ce2e-44ca-a5e3-f43dd3c9aeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fac152-3420-4b3f-9011-594a1e41179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeddings collected: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347ab054-3c3c-401c-a694-9b501c8f08a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Deleted corrupted zip file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88070c24-a7a4-4384-b979-a67e9bf9978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.10.0-cp310-cp310-win_amd64.whl (13.7 MB)\n",
      "Requirement already satisfied: packaging in e:\\clip_env\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in e:\\clip_env\\lib\\site-packages (from faiss-cpu) (2.2.4)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9dddd91-9f5b-428b-9431-38bb7ef4e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Using cached gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in e:\\clip_env\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Collecting orjson~=3.0\n",
      "  Using cached orjson-3.10.16-cp310-cp310-win_amd64.whl (133 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in e:\\clip_env\\lib\\site-packages (from gradio) (0.30.1)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Collecting typer<1.0,>=0.12\n",
      "  Using cached typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.2/95.2 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in e:\\clip_env\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in e:\\clip_env\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in e:\\clip_env\\lib\\site-packages (from gradio) (4.13.1)\n",
      "Collecting semantic-version~=2.0\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting ruff>=0.9.3\n",
      "  Using cached ruff-0.11.3-py3-none-win_amd64.whl (11.4 MB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\clip_env\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: packaging in e:\\clip_env\\lib\\site-packages (from gradio) (24.2)\n",
      "Collecting gradio-client==1.8.0\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "     -------------------------------------- 322.2/322.2 kB 6.6 MB/s eta 0:00:00\n",
      "Collecting groovy~=0.1\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic<2.12,>=2.0\n",
      "  Downloading pydantic-2.11.2-py3-none-any.whl (443 kB)\n",
      "     ------------------------------------- 443.3/443.3 kB 14.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in e:\\clip_env\\lib\\site-packages (from gradio) (2.2.4)\n",
      "Requirement already satisfied: httpx>=0.24.1 in e:\\clip_env\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.3/62.3 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in e:\\clip_env\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Collecting starlette<1.0,>=0.40.0\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in e:\\clip_env\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in e:\\clip_env\\lib\\site-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
      "Collecting websockets<16.0,>=10.0\n",
      "  Downloading websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "     -------------------------------------- 176.8/176.8 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\clip_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\clip_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\clip_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in e:\\clip_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\clip_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\clip_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\clip_env\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: filelock in e:\\clip_env\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in e:\\clip_env\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\clip_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\clip_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\clip_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.1\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "     -------------------------------------- 243.2/243.2 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.2/98.2 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: colorama in e:\\clip_env\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in e:\\clip_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\clip_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\clip_env\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\clip_env\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, websockets, typing-inspection, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, mdurl, groovy, ffmpy, click, annotated-types, aiofiles, uvicorn, starlette, pydantic, markdown-it-py, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 click-8.1.8 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.10.16 pydantic-2.11.2 pydantic-core-2.33.1 pydub-0.25.1 python-multipart-0.0.20 rich-14.0.0 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.1 tomlkit-0.13.2 typer-0.15.2 typing-inspection-0.4.0 uvicorn-0.34.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0acbdb-25e4-4cb4-b786-727b75737a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
