# -*- coding: utf-8 -*-
"""faissgradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ows71MIkfCX1O6uCPD_vc3nmLD9SJ3tu
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install gradio faiss-cpu torch torchvision

import zipfile, os, glob, torch

# STEP 1: Unzip your uploaded dataset from Drive
zip_path = "/content/drive/MyDrive/Flickr8k_Dataset.zip"
unzip_dir = "/content/images"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(unzip_dir)

print("‚úÖ Images extracted to:", unzip_dir)

'''
# STEP 2: Get embedding files
image_embedding_paths = sorted(glob.glob("/content/image_embeddings_batch_*.pt"))
text_embedding_paths = sorted(glob.glob("/content/text_embeddings_batch_*.pt"))
print(f"‚úÖ Found {len(image_embedding_paths)} image embedding files")
print(f"‚úÖ Found {len(text_embedding_paths)} text embedding files")

# STEP 3: Count total number of embeddings
total_embeddings = 0
for path in image_embedding_paths:
    emb = torch.load(path)
    total_embeddings += emb.shape[0]
print(f"üß† Total image embeddings available: {total_embeddings}")

# STEP 4: Load images in original order (not sorted)
ordered_files = []
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    for file in zip_ref.namelist():
        if file.lower().endswith((".jpg", ".jpeg", ".png")):
            ordered_files.append(os.path.join(unzip_dir, file))
original_image_files = [f for f in ordered_files if os.path.exists(f)]
original_image_files = original_image_files[:5000]
print(f"üñºÔ∏è Loaded {len(original_image_files)} images to match {total_embeddings} embeddings")
'''
# STEP 4: Load images in original order (not sorted)
ordered_files = []
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    for file in zip_ref.namelist():
        if file.lower().endswith((".jpg", ".jpeg", ".png")):
            ordered_files.append(os.path.join(unzip_dir, file))

# Filter only existing files
original_image_files = [f for f in ordered_files if os.path.exists(f)]

# Trim to match the number of indexed embeddings (FAISS index size)
original_image_files = original_image_files[:index.ntotal]
print(f"üñºÔ∏è Trimmed image list to match FAISS index: {len(original_image_files)} images")

import faiss

def load_embeddings(paths, limit=5000):
    embeddings = []
    for path in paths:
        emb = torch.load(path)
        if emb.numel() > 0:
            embeddings.append(emb)
        if sum(e.shape[0] for e in embeddings) >= limit:
            break
    return torch.cat(embeddings)[:limit]

image_embeddings = load_embeddings(image_embedding_paths)
text_embeddings = load_embeddings(text_embedding_paths)

index = faiss.IndexFlatL2(512)
index.add(image_embeddings.numpy())

from transformers import CLIPProcessor, CLIPModel
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import gradio as gr
from PIL import Image
import numpy as np

def search_by_text(text):
    if len(original_image_files) != index.ntotal:
        return [], f"‚ö†Ô∏è Mismatch: {len(original_image_files)} files vs {index.ntotal} index"

    inputs = processor(text=[text], return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        query_embedding = model.get_text_features(**inputs).cpu().numpy()
    D, I = index.search(query_embedding, k=10)

    result_images, result_info = [], []
    for j, (i, d) in enumerate(zip(I[0], D[0])):
        if i < len(original_image_files):
            result_images.append(original_image_files[i])
            result_info.append(f"Match {j+1}: Index {i}, Score {round(float(d), 2)}")
        else:
            result_info.append(f"‚ö†Ô∏è Invalid index: {i}")
    return result_images, "\n".join(result_info)

def search_by_image(img):
    if len(original_image_files) != index.ntotal:
        return [], f"‚ö†Ô∏è Mismatch: {len(original_image_files)} files vs {index.ntotal} index"

    inputs = processor(images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        query_embedding = model.get_image_features(**inputs).cpu().numpy()
    D, I = index.search(query_embedding, k=10)

    result_images, result_info = [], []
    for j, (i, d) in enumerate(zip(I[0], D[0])):
        if i < len(original_image_files):
            result_images.append(original_image_files[i])
            result_info.append(f"Match {j+1}: Index {i}, Score {round(float(d), 2)}")
        else:
            result_info.append(f"‚ö†Ô∏è Invalid index: {i}")
    return result_images, "\n".join(result_info)

with gr.Blocks() as demo:
    gr.Markdown("## üîç CLIP-based Visual Search Engine")
    with gr.Tab("Search by Text"):
        txt_input = gr.Textbox(label="Describe an image (e.g., 'a dog running')")
        txt_gallery = gr.Gallery(label="Top Matches", columns=2)
        txt_result_info = gr.Textbox(label="Matched Indices & Scores")
        txt_btn = gr.Button("Search")
        txt_btn.click(fn=search_by_text, inputs=txt_input, outputs=[txt_gallery, txt_result_info])

    with gr.Tab("Search by Image"):
        img_input = gr.Image(type="pil", label="Upload an image", interactive=True)
        img_gallery = gr.Gallery(label="Top Matches", columns=2)
        img_result_info = gr.Textbox(label="Matched Indices & Scores")
        img_btn = gr.Button("Search")
        img_btn.click(fn=search_by_image, inputs=img_input, outputs=[img_gallery, img_result_info])

demo.launch(share=True, debug=True, allowed_paths=["/content/images"])

